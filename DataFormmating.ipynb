{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cffb8a-c0f2-40bd-84e7-7b5a0bd7f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pickle import dump\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_path = os.getcwd() +'/data/'# Initial folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c47b24da-a3af-4b62-beb4-ea8b378e40d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating one\n",
      "Download the data\n",
      "unzipping the data\n"
     ]
    }
   ],
   "source": [
    "# setup path to a data folder\n",
    "image_path= Path(dataset_path)\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} directory is already existed ... skipping download')\n",
    "else:\n",
    "    print('creating one')\n",
    "    image_path.mkdir(parents=True,exist_ok=True)\n",
    "    # download the data\n",
    "    with open(image_path /'data.zip','wb') as f: # wb: write binary\n",
    "        request=requests.get('https://github.com/hunissweet/Begin_SOMARO/raw/main/data.zip')\n",
    "        \n",
    "        print('Download the data')\n",
    "        f.write(request.content)\n",
    "\n",
    "    with zipfile.ZipFile(image_path/'data.zip','r') as zip_ref:\n",
    "        print('unzipping the data')\n",
    "        zip_ref.extractall(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5244490a-6561-461d-9fde-188aa73b8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='data/'  #new data set path after un zipping the file\n",
    "train_data_dir = dataset_path + 'train/' # Folder for original data\n",
    "test_data_dir  = dataset_path + 'test/'  # Folder for original data \n",
    "train_out_dir  = dataset_path + 'train_out/' # After formmating\n",
    "test_out_dir   = dataset_path + 'test_out/'  # After formmating\n",
    "scaler_out_dir = dataset_path + 'scale_er/'  # pickle file for scaler => This file is processor for scaling, such as min-max, standarziation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fb4ff7-f53b-40eb-91b5-88da8ce6f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir path : data/train/\n",
      "number of dir name : 5\n",
      "Filenames []\n",
      "Dir path : data/train/data_sample_2022-02-21-09-58-49\n",
      "number of dir name : 0\n",
      "Filenames ['marker.csv', 'meta_data.csv', 'robot_state.csv', 'slip_label.csv', 'xela_sensor1.csv', 'xela_sensor2.csv']\n",
      "\n",
      "\n",
      "Dir path : data/test/\n",
      "number of dir name : 2\n",
      "Filenames []\n",
      "Dir path : data/test/data_sample_2022-02-22-11-23-11\n",
      "number of dir name : 0\n",
      "Filenames ['marker.csv', 'meta_data.csv', 'robot_state.csv', 'slip_label.csv', 'xela_sensor1.csv', 'xela_sensor2.csv']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def walk_through_dir(dir_path):\n",
    "    for dirpath,dirnames,filenames in os.walk(dir_path):\n",
    "        print(f'Dir path : {(dirpath)}\\nnumber of dir name : {len(dirnames)}\\nFilenames {filenames}')\n",
    "        if len(filenames)>0:\n",
    "            print('\\n')\n",
    "            break\n",
    "walk_through_dir(train_data_dir)\n",
    "\n",
    "walk_through_dir(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24fa95e-80a6-4a2e-80ec-906e29f9ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper parameter ###\n",
    "smooth = False\n",
    "filter_kalman = True\n",
    "image = False\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "context_length = 10\n",
    "horrizon_length = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370277e9-dd2b-46ff-9e75-27e5d937057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'slip_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 157\u001b[0m\n\u001b[0;32m    153\u001b[0m     df\u001b[38;5;241m.\u001b[39mcreate_map()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 152\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m df \u001b[38;5;241m=\u001b[39m data_formatter()\n\u001b[0;32m    151\u001b[0m df\u001b[38;5;241m.\u001b[39mload_file_names()\n\u001b[1;32m--> 152\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m df\u001b[38;5;241m.\u001b[39mcreate_map()\n",
      "Cell \u001b[1;32mIn[9], line 93\u001b[0m, in \u001b[0;36mdata_formatter.scale_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_test\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[1;32m---> 93\u001b[0m     tactile, robot, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_file_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_data_tactile \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tactile)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_data_robot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(robot)\n",
      "Cell \u001b[1;32mIn[9], line 129\u001b[0m, in \u001b[0;36mdata_formatter.load_file_data\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    125\u001b[0m             kf \u001b[38;5;241m=\u001b[39m KalmanFilter(initial_state_mean\u001b[38;5;241m=\u001b[39mtactile_data[\u001b[38;5;241m0\u001b[39m, i, :], n_dim_obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, initial_state_covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m16\u001b[39m), observation_covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m    126\u001b[0m             tactile_data[:, i, :], _ \u001b[38;5;241m=\u001b[39m kf\u001b[38;5;241m.\u001b[39mfilter(tactile_data[:, i, :])\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tactile_data, robot_task_space, meta_data, \u001b[43mslip_label\u001b[49m, fail_label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'slip_label' is not defined"
     ]
    }
   ],
   "source": [
    "class data_formatter:\n",
    "    def __init__(self):\n",
    "        self.files_train = []\n",
    "        self.files_test = []\n",
    "        self.full_data_tactile = []\n",
    "        self.full_data_robot = []\n",
    "        self.smooth = smooth\n",
    "        self.filter_kalman = filter_kalman\n",
    "        self.context_length = context_length\n",
    "        self.horrizon_length = horrizon_length\n",
    "\n",
    "    def create_map(self):\n",
    "        for stage in [train_out_dir, test_out_dir]:\n",
    "            self.path_file = []\n",
    "            index_to_save = 0\n",
    "            print(stage)\n",
    "            if stage == train_out_dir:\n",
    "                files_to_run = self.files_train\n",
    "            else:\n",
    "                files_to_run = self.files_test\n",
    "\n",
    "            for experiment_number, file in tqdm(enumerate(files_to_run)):\n",
    "                if stage == test_out_dir:\n",
    "                    path_save = stage + \"/test_trial_\" + str(experiment_number) + '/'\n",
    "                    os.mkdir(path_save)\n",
    "                    self.path_file = []\n",
    "                    index_to_save = 0\n",
    "                else:\n",
    "                    path_save = stage\n",
    "\n",
    "                tactile, robot, meta, slip, failure = self.load_file_data(file)\n",
    "\n",
    "                # scale the data\n",
    "                for index, (standard_scaler, min_max_scalar) in enumerate(zip(self.tactile_standard_scaler, self.tactile_min_max_scalar)):\n",
    "                    tactile[:, index] = standard_scaler.transform(tactile[:, index])\n",
    "                    tactile[:, index] = min_max_scalar.transform(tactile[:, index])\n",
    "                \n",
    "                for index, min_max_scalar in enumerate(self.robot_min_max_scalar):\n",
    "                    robot[:, index] = np.squeeze(min_max_scalar.transform(robot[:, index].reshape(-1, 1)))\n",
    "                \n",
    "\n",
    "                sequence_length = self.context_length + self.horrizon_length\n",
    "                for time_step in range(len(tactile) - sequence_length):\n",
    "                    robot_data_sequence = [robot[time_step + t] for t in range(sequence_length)]\n",
    "                    tactile_data_sequence     = [tactile[time_step + t] for t in range(sequence_length)]\n",
    "                    experiment_data_sequence  = experiment_number\n",
    "                    time_step_data_sequence   = [time_step + t for t in range(sequence_length)]\n",
    "                    slip_label_sequence = slip[time_step + sequence_length - 1]\n",
    "                    failure_label_sequence = failure[time_step + sequence_length - 1]\n",
    "                  \n",
    "                    ###################################### Save the data and add to the map ###########################################\n",
    "                    np.save(path_save + 'robot_data_' + str(index_to_save), robot_data_sequence)\n",
    "                    np.save(path_save + 'tactile_data_sequence_' + str(index_to_save), tactile_data_sequence)\n",
    "                    np.save(path_save + 'experiment_number_' + str(index_to_save), experiment_data_sequence)\n",
    "                    np.save(path_save + 'time_step_data_' + str(index_to_save), time_step_data_sequence)\n",
    "                    np.save(path_save + 'trial_meta_' + str(index_to_save), np.array(meta))\n",
    "                    np.save(path_save + 'slip_data_' + str(index_to_save), np.array(slip_label_sequence))\n",
    "                    np.save(path_save + 'failure_data_' + str(index_to_save), np.array(failure_label_sequence))\n",
    "                    ref = []\n",
    "                    ref.append('robot_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('tactile_data_sequence_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('experiment_number_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('time_step_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('trial_meta_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('slip_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('failure_data_' + str(index_to_save) + '.npy')\n",
    "                    self.path_file.append(ref)\n",
    "                    index_to_save += 1\n",
    "\n",
    "                if stage == test_out_dir:\n",
    "                    self.test_no = experiment_number\n",
    "                    self.save_map(path_save, test=True)\n",
    "\n",
    "            self.save_map(path_save)\n",
    "\n",
    "    def save_map(self, path, test=False):\n",
    "        if test:\n",
    "            with open(path + '/map_' + str(self.test_no) + '.csv', 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(['robot_data_path', 'tactile_data_sequence', 'experiment_number', 'time_steps', 'meta', 'slip', 'failure'])\n",
    "                for row in self.path_file:\n",
    "                    writer.writerow(row)\n",
    "        else:\n",
    "            with open(path + '/map.csv', 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(['robot_data_path', 'tactile_data_sequence', 'experiment_number', 'time_steps', 'meta', 'slip', 'failure'])\n",
    "                for row in self.path_file:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    def scale_data(self):\n",
    "        files = self.files_train + self.files_test\n",
    "        for file in tqdm(files):\n",
    "            tactile, robot, _, _, _ = self.load_file_data(file)\n",
    "            self.full_data_tactile += list(tactile)\n",
    "            self.full_data_robot += list(robot)\n",
    "\n",
    "        self.full_data_tactile = np.array(self.full_data_tactile)\n",
    "        self.full_data_robot = np.array(self.full_data_robot)\n",
    "\n",
    "        self.robot_min_max_scalar = [preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(self.full_data_robot[:, feature].reshape(-1, 1)) for feature in range(2)]\n",
    "        self.tactile_standard_scaler = [preprocessing.StandardScaler().fit(self.full_data_tactile[:, feature]) for feature in range(3)]\n",
    "        self.tactile_min_max_scalar = [preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(self.tactile_standard_scaler[feature].transform(self.full_data_tactile[:, feature])) for feature in range(3)]\n",
    "\n",
    "        self.save_scalars()\n",
    "\n",
    "    def load_file_data(self, file):\n",
    "        robot_state = np.array(pd.read_csv(file + '/robot_state.csv', header=None))\n",
    "        meta_data = np.array(pd.read_csv(file + '/meta_data.csv', header=None))\n",
    "        xela_sensor = pd.read_csv(file + '/xela_sensor1.csv')\n",
    "        slip_label = pd.read_csv(file + '/slip_label.csv')['slip']\n",
    "        fail_label = pd.read_csv(file + '/slip_label.csv')['fail']\n",
    "\n",
    "        xela_sensor = np.array(xela_sensor.sub(xela_sensor.loc[0]))\n",
    "\n",
    "        robot_task_space = np.array([[state[-3], state[-2]] for state in robot_state[1:]]).astype(float)\n",
    "\n",
    "        tactile_data_split = [np.array(xela_sensor[:, [i for i in range(feature, 48, 3)]]).astype(float) for feature in range(3)]\n",
    "        tactile_data = np.array([[tactile_data_split[feature][ts] for feature in range(3)] for ts in range(tactile_data_split[0].shape[0])]) #shape: lenx3x16\n",
    "       \n",
    "        if self.filter_kalman:\n",
    "            for i in range(tactile_data.shape[1]):\n",
    "                if i < 2: # x, y initial cov = 4\n",
    "                    kf = KalmanFilter(initial_state_mean=tactile_data[0, i, :], n_dim_obs=16, initial_state_covariance=1*np.eye(16), observation_covariance=4*np.eye(16))\n",
    "                    tactile_data[:, i, :], _ = kf.filter(tactile_data[:, i, :])\n",
    "                elif i ==2: # z initial cov = 8\n",
    "                    kf = KalmanFilter(initial_state_mean=tactile_data[0, i, :], n_dim_obs=16, initial_state_covariance=1*np.eye(16), observation_covariance=8*np.eye(16))\n",
    "                    tactile_data[:, i, :], _ = kf.filter(tactile_data[:, i, :])\n",
    "        \n",
    "\n",
    "        return tactile_data, robot_task_space, meta_data, slip_label, fail_label\n",
    "\n",
    "    def load_file_names(self):\n",
    "        self.files_train = glob.glob(train_data_dir + '/*')\n",
    "        self.files_test = glob.glob(test_data_dir + '/*')\n",
    "\n",
    "    def save_scalars(self):\n",
    "        # save the scalars\n",
    "        dump(self.tactile_standard_scaler[0], open(scaler_out_dir + 'tactile_standard_scaler_x.pkl', 'wb'))\n",
    "        dump(self.tactile_standard_scaler[1], open(scaler_out_dir + 'tactile_standard_scaler_y.pkl', 'wb'))\n",
    "        dump(self.tactile_standard_scaler[2], open(scaler_out_dir + 'tactile_standard_scaler_z.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[0], open(scaler_out_dir + 'tactile_min_max_scalar_x.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[1], open(scaler_out_dir + 'tactile_min_max_scalar_y.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[2], open(scaler_out_dir + 'tactile_min_max_scalar_z.pkl', 'wb'))\n",
    "\n",
    "\n",
    "        dump(self.robot_min_max_scalar[0], open(scaler_out_dir + 'robot_min_max_scalar_vx.pkl', 'wb'))\n",
    "        dump(self.robot_min_max_scalar[1], open(scaler_out_dir + 'robot_min_max_scalar_vy.pkl', 'wb'))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = data_formatter()\n",
    "    df.load_file_names()\n",
    "    df.scale_data()\n",
    "    df.create_map()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3336f69-a8b4-47e6-843b-831d705fde31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello github\n"
     ]
    }
   ],
   "source": [
    "# NEW line for testing GITHUB\n",
    "print('Hello github')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff4c2e-e923-41c2-abd3-4a9a7917be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
